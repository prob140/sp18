<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/21_02_Beta_Binomial_Distribution.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Beta-Binomial-Distribution">The Beta-Binomial Distribution<a class="anchor-link" href="#The-Beta-Binomial-Distribution">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As in the previous section, let $X$ have the beta $(r, s)$ prior, and given $X = p$ let the $S_n$ be the number of heads in the first $n$ tosses of a $p$-coin.</p>
<p>All the calculations we carried out in the previous section were under the condition that $S_n = k$, but we never needed to find the probability of this event. It was part of the constant that made the posterior density of $X$ integrate to 1.</p>
<p>We can now find $P(S_n = k)$ by writing the posterior density in two ways:</p>
<ul>
<li>By recalling that it is the beta $(r+k, s+n-k)$ density:</li>
</ul>
$$
f_{X \vert S_n=k} (p) ~ = ~ C(r+k, s+n-k)p^{r+k-1}(1-p)^{s+n-k-1}, ~~~~ 0 &lt; p &lt; 1
$$<ul>
<li>By using Bayes' Rule:</li>
</ul>
$$
f_{X \vert S_n=k} (p) ~ = ~ \frac{C(r, s) p^{r-1}(1-p)^{s-1} \cdot \binom{n}{k} p^k (1-p)^{n-k}}{P(S_n = k)}, ~~~~ 0 &lt; p &lt; 1
$$<p>Now equate constants:</p>
$$
\frac{C(r, s) \binom{n}{k}}{P(S_n = k)} ~ = ~ C(r+k, s+n-k)
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Beta-Binomial-Probabilities">Beta-Binomial Probabilities<a class="anchor-link" href="#Beta-Binomial-Probabilities">¶</a></h3><p>So for $k$ in the range 0 through $n$,</p>
$$
P(S_n = k) ~ = ~  \binom{n}{k} \frac{C(r, s)}{C(r+k, s+n-k)}
$$<p>where $C(r,s)$ is the constant in the beta $(r, s)$ density, given by</p>
$$
C(r, s) ~ = ~ \frac{\Gamma(r+s)}{\Gamma(r)\Gamma(s)}
$$<p>This discrete distribution is called the <em>beta-binomial</em> distribution with parameters $r$, $s$, and $n$. It is the distribution of the number of heads in $n$ tosses of a coin that lands heads with a probability picked according to the beta $(r, s)$ distribution.</p>
<p>One $(r, s)$ pair is particularly interesting: $r = s = 1$. That's the case when $X$ has the uniform prior. The distribution of $S_n$ reduces to</p>
$$
P(S_n = k ) ~ = ~ \frac{n!}{k!(n-k)!} \cdot \frac{1!}{0!0!} \cdot \frac{k!(n-k)!}{(n+1)!} ~ = ~ \frac{1}{n+1}
$$<p>There's no $k$ in the answer! The conclusion is that if you choose $p$ uniformly between 0 and 1 and toss a $p$-coin $n$ times, <em>the distribution of the number of heads is uniform</em> on $\{ 0, 1, 2, \ldots, n\}$.</p>
<p>If you choose $p$ uniformly between 0 and 1, then for the conditional distribution of $S_n$ given that $p$ was the selected value is binomial $(n, p)$. But the unconditional distribution of $S_n$ is uniform.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Checking-by-Integration">Checking by Integration<a class="anchor-link" href="#Checking-by-Integration">¶</a></h3><p>If you prefer, you can find the distribution of $S_n$ directly, by conditioning on $X$.</p>
\begin{align*}
P(S_n = k) ~ &amp;= \int_0^1 P(S_n = k \mid X = p)f_X(p)dp \\ \\
&amp;= ~ \int_0^1 \binom{n}{k} p^k(1-p)^{n-k}C(r, s)p^{r-1}(1-p)^{s-1}dp \\ \\
&amp;= ~ \binom{n}{k} C(r, s) \int_0^1 p^{r+k-1}(1-p)^{s+n-k-1} dp \\ \\
&amp;= ~ \binom{n}{k} C(r, s) \frac{1}{C(r+k, s+n-k)}
\end{align*}</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Expectation">Expectation<a class="anchor-link" href="#Expectation">¶</a></h3><p>Given $X = p$, the conditional distribution of $S_n$ is binomial $(n, p)$. Therefore</p>
$$
E(S_n \mid X = p) ~ = ~ np
$$<p>or, equivalently,
$$
E(S_n \mid X) ~ = ~ nX
$$
By iteration,
$$
E(S_n) ~ = ~ E(nX) ~ = ~ nE(X) ~ = ~ n\frac{r}{r+s}
$$</p>
<p>The expected proportion of heads in $n$ tosses is
$$
E\big{(} \frac{S_n}{n} \big{)} ~ = ~ \frac{r}{r+s}
$$</p>
<p>which is the expectation of the prior distribution of $X$.</p>
<p>In the next section we will examine the long run behavior of this random proportion.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Endnote">Endnote<a class="anchor-link" href="#Endnote">¶</a></h3><p>The unconditional probability $P(S_n = k)$ appeared in the denominator of our calculation of the posterior density of $X$ given $S_n$. Because of the simplifications that result from using conjugate priors, we were able to calculate the denominator in a couple of different ways. But often the calculation can be intractable, especially in high dimensional settings. Methods of dealing with this problem are covered in more advanced courses.</p></div></div></div>