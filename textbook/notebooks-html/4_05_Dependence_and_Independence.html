<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/4_05_Dependence_and_Independence.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dependence-and-Independence">Dependence and Independence<a class="anchor-link" href="#Dependence-and-Independence">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Conditional distributions help us formalize our intuitive ideas about whether two random variables are independent of each other. Let $X$ and $Y$ be two random variables, and suppose we are given the value of $X$. Does that change our opinion about $Y$? If the answer is yes, then we will say that $X$ and $Y$ are dependent. If the answer is no, no matter what the given value of $X$ is, then we will say that $X$ and $Y$ are independent.</p>
<p>Let's start with some examples and then move to precise definitions and results.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dependence">Dependence<a class="anchor-link" href="#Dependence">¶</a></h3><p>Here is the joint distribution of two random variables $X$ and $Y$. From this, what can we say about whether $X$ and $Y$ are dependent or independent?</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist1</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X=0</th>
      <th>X=1</th>
      <th>X=2</th>
      <th>X=3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Y=3</th>
      <td>0.037037</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>Y=2</th>
      <td>0.166667</td>
      <td>0.055556</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>Y=1</th>
      <td>0.250000</td>
      <td>0.166667</td>
      <td>0.027778</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>Y=0</th>
      <td>0.125000</td>
      <td>0.125000</td>
      <td>0.041667</td>
      <td>0.00463</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see at once that if $X = 3$ then $Y$ can only be 0, whereas if $X = 2$ then $Y$ can be either 1 or 2. Knowing the value of $X$ changes the distribution of $Y$. That's dependence.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is an example in which you can't quickly determine dependence or independence by just looking at the possible values.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist2</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X=3</th>
      <th>X=4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Y=7</th>
      <td>0.3</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>Y=6</th>
      <td>0.2</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>Y=5</th>
      <td>0.1</td>
      <td>0.1</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But you can tell by looking at the conditional distributions of $Y$ given $X$. They are different.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist2</span><span class="o">.</span><span class="n">conditional_dist</span><span class="p">(</span><span class="s1">'Y'</span><span class="p">,</span> <span class="s1">'X'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dist. of Y | X=3</th>
      <th>Dist. of Y | X=4</th>
      <th>Marginal of Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Y=7</th>
      <td>0.500000</td>
      <td>0.25</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>Y=6</th>
      <td>0.333333</td>
      <td>0.50</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>Y=5</th>
      <td>0.166667</td>
      <td>0.25</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>Sum</th>
      <td>1.000000</td>
      <td>1.00</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It follows (and you should try to prove this), that at least some of the conditional distributions of $X$ given the different values of $Y$ will also be different from each other and from the marginal of $X$.</p>
<p>Notice that not all the conditional distributions are different. The conditional distribution of $X$ given $Y=5$ is the same as the conditional distribution of $X$ given $Y=6$. But given $Y=7$, the conditional distribution changes. $X$ and $Y$ are dependent.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist2</span><span class="o">.</span><span class="n">conditional_dist</span><span class="p">(</span><span class="s1">'X'</span><span class="p">,</span> <span class="s1">'Y'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X=3</th>
      <th>X=4</th>
      <th>Sum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Dist. of X | Y=7</th>
      <td>0.75</td>
      <td>0.25</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of X | Y=6</th>
      <td>0.50</td>
      <td>0.50</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of X | Y=5</th>
      <td>0.50</td>
      <td>0.50</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Marginal of X</th>
      <td>0.60</td>
      <td>0.40</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Independence">Independence<a class="anchor-link" href="#Independence">¶</a></h3></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is a joint distribution table in which you can't immediately tell whether there is dependence.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist3</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X=0</th>
      <th>X=1</th>
      <th>X=2</th>
      <th>X=3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Y=4</th>
      <td>0.000096</td>
      <td>0.000289</td>
      <td>0.000289</td>
      <td>0.000096</td>
    </tr>
    <tr>
      <th>Y=3</th>
      <td>0.001929</td>
      <td>0.005787</td>
      <td>0.005787</td>
      <td>0.001929</td>
    </tr>
    <tr>
      <th>Y=2</th>
      <td>0.014468</td>
      <td>0.043403</td>
      <td>0.043403</td>
      <td>0.014468</td>
    </tr>
    <tr>
      <th>Y=1</th>
      <td>0.048225</td>
      <td>0.144676</td>
      <td>0.144676</td>
      <td>0.048225</td>
    </tr>
    <tr>
      <th>Y=0</th>
      <td>0.060282</td>
      <td>0.180845</td>
      <td>0.180845</td>
      <td>0.060282</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But look what happens when you condition $X$ on $Y$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist3</span><span class="o">.</span><span class="n">conditional_dist</span><span class="p">(</span><span class="s1">'X'</span><span class="p">,</span> <span class="s1">'Y'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X=0</th>
      <th>X=1</th>
      <th>X=2</th>
      <th>X=3</th>
      <th>Sum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Dist. of X | Y=4</th>
      <td>0.125</td>
      <td>0.375</td>
      <td>0.375</td>
      <td>0.125</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of X | Y=3</th>
      <td>0.125</td>
      <td>0.375</td>
      <td>0.375</td>
      <td>0.125</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of X | Y=2</th>
      <td>0.125</td>
      <td>0.375</td>
      <td>0.375</td>
      <td>0.125</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of X | Y=1</th>
      <td>0.125</td>
      <td>0.375</td>
      <td>0.375</td>
      <td>0.125</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Dist. of X | Y=0</th>
      <td>0.125</td>
      <td>0.375</td>
      <td>0.375</td>
      <td>0.125</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>Marginal of X</th>
      <td>0.125</td>
      <td>0.375</td>
      <td>0.375</td>
      <td>0.125</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All the rows are the same. That is, all the conditional distributions of $X$ given different values of $Y$ are the same, and hence are the same as the marginal of $X$ too.</p>
<p>Given the value of $Y$, the probabilities for $X$ don't change at all. That's independence.</p>
<p>You could have drawn the same conclusion by conditioning $Y$ on $X$:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dist3</span><span class="o">.</span><span class="n">conditional_dist</span><span class="p">(</span><span class="s1">'Y'</span><span class="p">,</span> <span class="s1">'X'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Dist. of Y | X=0</th>
      <th>Dist. of Y | X=1</th>
      <th>Dist. of Y | X=2</th>
      <th>Dist. of Y | X=3</th>
      <th>Marginal of Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Y=4</th>
      <td>0.000772</td>
      <td>0.000772</td>
      <td>0.000772</td>
      <td>0.000772</td>
      <td>0.000772</td>
    </tr>
    <tr>
      <th>Y=3</th>
      <td>0.015432</td>
      <td>0.015432</td>
      <td>0.015432</td>
      <td>0.015432</td>
      <td>0.015432</td>
    </tr>
    <tr>
      <th>Y=2</th>
      <td>0.115741</td>
      <td>0.115741</td>
      <td>0.115741</td>
      <td>0.115741</td>
      <td>0.115741</td>
    </tr>
    <tr>
      <th>Y=1</th>
      <td>0.385802</td>
      <td>0.385802</td>
      <td>0.385802</td>
      <td>0.385802</td>
      <td>0.385802</td>
    </tr>
    <tr>
      <th>Y=0</th>
      <td>0.482253</td>
      <td>0.482253</td>
      <td>0.482253</td>
      <td>0.482253</td>
      <td>0.482253</td>
    </tr>
    <tr>
      <th>Sum</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Independence-of-Two-Events">Independence of Two Events<a class="anchor-link" href="#Independence-of-Two-Events">¶</a></h3><p>The concept of independence seems intuitive, but it is possible to run into a lot of trouble by not being careful about its definition. So let's define it formally.</p>
<p>There are two equivalent definitions of the independence of two events. The first encapsulates the main idea of independence, and the second is useful for calculation.</p>
<p>Two events $A$ and $B$ are <em>independent</em> if $P(B \mid A) = P(B)$. Equivalently, $A$ and $B$ are independent if $P(AB) = P(A)P(B)$.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Independence-of-Two-Random-Variables">Independence of Two Random Variables<a class="anchor-link" href="#Independence-of-Two-Random-Variables">¶</a></h3><p>What we have observed in the examples of this section can be turned into a formal definition of independence.</p>
<p>Two random variables $X$ and $Y$ are <em>independent</em> if for every value $x$ of $X$ and $y$ of $Y$,
$$
P(Y = y \mid X = x) = P(Y = y)
$$
That is, no matter what the given $x$ is, the conditional distribution of $Y$ is the same as if we didn't know that $X=x$.</p>
<p>An equivalent definition in terms of the independence of events is that for any values of $x$ and $y$, the events $\{ X=x\}$ and $\{Y=y\}$ are independent.</p>
<p>That is, $X$ and $Y$ are independent if for any values $x$ of $X$ and $y$ of $Y$,</p>
$$
P(X = x, Y = y) ~ = ~ P(X=x)P(Y=y) 
$$<p>Independence simplifies the conditional probabilities in the multiplication rule.</p>
<p>It is a fact that if $X$ and $Y$ are independent random variables, then any event determined by $X$ is independent of any event determined by $Y$. For example, if $X$ and $Y$ are independent and $x$ is a number, then $\{X=x\}$ is independent of $\{Y&gt;x\}$. Also, any function of $X$ is independent of any function of $Y$.</p>
<p>You can prove these facts by partitioning and then using the definition of independence. The proofs are routine but somewhat labor intensive. You are welcome to just accept the facts if you don't want to prove them.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mutual-Independence">Mutual Independence<a class="anchor-link" href="#Mutual-Independence">¶</a></h3><p>Events $A_1, A_2, \ldots A_n$ are <em>mutually independent</em> (or <em>independent</em> for short) if given that any subset of the events has occurred, the conditional chances of all other subsets remain unchanged.</p>
<p>That's quite a mouthful. In practical terms it means that it doesn't matter which of the events you know have happened; chances involving the remaining events are unchanged.</p>
<p>In terms of random variables, $X_1, X_2, \ldots , X_n$ are independent if given the values of any subset, chances of events determined by the remaining variables are unchanged.</p>
<p>In practice, this just formalizes statements such as "results of different tosses of a coin are independent" or "draws made at random with replacement are independent".</p>
<p>Try not to become inhibited by the formalism. Notice how the theory not only supports intuition but also develops it. You can expect your probabilistic intuition to be much sharper at the end of this course than it is now!</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id='"i.i.d."-Random-Variables'>"i.i.d." Random Variables<a class="anchor-link" href='#"i.i.d."-Random-Variables'>¶</a></h3><p>If random variables are mutually independent and identically distributed, they are called "i.i.d." That's one of the most famous acronyms in probability theory. You can think of i.i.d. random variables as draws with replacement from a population, or as the results of independent replications of the same experiment.</p>
<p>Suppose the distribution of $X$ is given by
$$
P(X = i) = p_i, ~~~ i = 1, 2, \ldots, n
$$
where $\sum_{i=1}^n p_i = 1$. Now let $X$ and $Y$ be i.i.d. What is $P(X = Y)$? We'll answer this question by using the fundamental method, now in random variable notation.</p>
\begin{align*}
P(X = Y) &amp;= \sum_{i=1}^n P(X = i, Y = i) ~~~ \text{(partitioning)} \\
&amp;= \sum_{i=1}^n P(X = i)P(Y = i) ~~~ \text{(independence)} \\
&amp;= \sum_{i=1}^n p_i \cdot p_i ~~~ \text{(identical distributions)} \\
&amp;= \sum_{i=1}^n p_i^2
\end{align*}<p>The last expression is easy to calculate if you know the numerical values of all the $p_i$.</p>
<p>In the same way,</p>
\begin{align*}
P(Y &gt; X) &amp;= \sum_{i=1}^n P(X = i, Y &gt; i) \\
&amp;= \sum_{i=1}^n P(X = i)P(Y &gt; i) \\
&amp;= \sum_{i=1}^{n-1} P(X = i)P(Y &gt; i)
\end{align*}<p>because $P(Y &gt; n) = 0$. Now for each $i &lt; n$, $P(Y &gt; i) = P(X &gt; i) = \sum_{j=i+1}^n p_j$. Call this sum $b_i$ for "bigger than $i$". Then</p>
\begin{align*}
P(Y &gt; X) &amp;= \sum_{i=1}^{n-1} P(X = i)P(Y &gt; i) \\
&amp;= \sum_{i=1}^{n-1} p_ib_i
\end{align*}<p>This is also a straightforward calculation if you know all the $p_i$. For $n=4$ it boils down to
$$
p_1 \cdot(p_2 + p_3 + p_4) ~~ + ~~ p_2 \cdot (p_3 + p_4) ~~ + ~~ p_3 \cdot p_4
$$</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>