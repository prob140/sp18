<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/13_01_Properties_of_Covariance.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Properties-of-Covariance">Properties of Covariance<a class="anchor-link" href="#Properties-of-Covariance">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's examine how covariance behaves. In the next two sections we will use our observations to calculate variances of sample sums.</p>
<p>Establishing properties of covariance involves simple observations and routine algebra. We have done some of it below, and we expect that you can fill in the rest.</p>
<p>Recall that the covariance of $X$ and $Y$ is</p>
$$
Cov(X, Y) ~ = ~ E(D_XD_Y) ~ = ~ E[(X - \mu_X)(Y - \mu_Y)]
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Constants-Don't-Vary">Constants Don't Vary<a class="anchor-link" href="#Constants-Don't-Vary">¶</a></h3><p>That title has a "duh" quality. But it's still worth noting that for any constant $c$,
$$
Cov(X, c) = 0
$$</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Variance-is-a-Covariance">Variance is a Covariance<a class="anchor-link" href="#Variance-is-a-Covariance">¶</a></h3><p>Covariance is an extension of the concept of variance, because</p>
$$
Var(X) = E(D_X^2) = E(D_XD_X) = Cov(X, X)
$$<p>The variance of $X$ is the covariance of $X$ and itself.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Covariance-is-Symmetric">Covariance is Symmetric<a class="anchor-link" href="#Covariance-is-Symmetric">¶</a></h3><p>Clearly $Cov(Y, X) = Cov(X, Y)$. It follows that</p>
$$
Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y) = Var(X) + Var(Y) + Cov(X, Y) + Cov(Y, X)
$$<p>This way of thinking about the variance of a sum will be useful later.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Covariance-and-Expected-Products">Covariance and Expected Products<a class="anchor-link" href="#Covariance-and-Expected-Products">¶</a></h3><p>Covariance <em>is</em> an expected product: it is the expected product of deviations. It can also be written in terms of the expected product of $X$ and $Y$, as follows.</p>
\begin{align*}
Cov(X, Y) &amp;= E[(X - \mu_X)(Y - \mu_Y)] \\
&amp;= E(XY) - E(X)\mu_Y - \mu_XE(Y) + \mu_X\mu_Y \\
&amp;= E(XY) - \mu_X\mu_Y
\end{align*}<p>So covariance is the <em>mean of the product minus the product of the means</em>. Take $X = Y$ to get the familiar fact that variance is the mean of the square minus the square of the mean.</p>
<p>This result simplifies proofs of facts about covariance, as you will see below. But as a computational tool, it is only useful when the distributions of $X$ and $Y$ are very simple – for example, when each has just a few possible values. In other calculations of covariance it is rarely a good idea to try to use this result. Rather, we will use the properties below.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Addition-Rule">Addition Rule<a class="anchor-link" href="#Addition-Rule">¶</a></h3><p>A routine application of the property above shows that for any random variables $X$, $Y$, and $Z$,</p>
$$
Cov(X+Y, Z) ~ = ~ Cov(X, Z) + Cov(Y, Z)
$$<p>Just write $Cov(X+Y, Z) = E[(X+Y)Z] - E(X+Y)E(Z)$, expand both products, and collect terms.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Main-Property:-Bilinearity">The Main Property: Bilinearity<a class="anchor-link" href="#The-Main-Property:-Bilinearity">¶</a></h3><p>This is the key to using covariance. First, easy algebra shows that for constants $a$ and $b$,
$$
Cov(aX, bY) = abCov(X, Y)
$$</p>
<p>Put this together with the addition rule to get</p>
$$
Cov(aX + bY, cZ) = acCov(X, Z) + bcCov(Y, Z)
$$<p>You can see that covariance behaves like products. By induction,</p>
$$
Cov(\sum_{i=1}^n a_iX_i, \sum_{j=1}^m b_jY_j) ~ = ~
\sum_{i=1}^n\sum_{j=1}^m a_ib_jCov(X_i, Y_j)
$$<p>That might look intimidating, but in fact this property greatly simplifies calculation. It says that you can expand covariance like the product of two sums. For example,</p>
$$
Cov(10X - Y, 3Y + Z) = 30Cov(X, Y) + 10Cov(X, Z) - 3Cov(Y, Y) - Cov(Y, Z)
$$<p>You can replace $Cov(Y, Y)$ by $Var(Y)$.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These properties simplify calculations. But they don't give a sense of what covariance means. Indeed it's not easy to understand covariance physically as it has nasty units: for example, if $X$ is a weight in kilograms and $Y$ is height in centimeters, then the units of covariance is <em>kilogram centimeters</em>. Later in the course we will see how to normalize covariance to get the <em>correlation coefficient</em> that you used so often in Data 8. For now, here is a property that begins to show you that covariance can be helpful in quantifying dependence and independence.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Independent-Implies-Uncorrelated">Independent Implies Uncorrelated<a class="anchor-link" href="#Independent-Implies-Uncorrelated">¶</a></h3><p>Let $X$ and $Y$ be independent. Then</p>
\begin{align*}
E(XY) &amp;= \sum_x\sum_y xyP(X=x, Y=y) ~~~~~~ \text{(expectation of a function)} \\
&amp;= \sum_x\sum_y xyP(X=x)P(Y=y) ~~~~ \text{(independence)} \\
&amp;= \sum_x xP(X=x) \sum_y yP(Y=y) \\
&amp;= E(X)E(Y)
\end{align*}<p>Therefore if $X$ and $Y$ are independent, then $Cov(X, Y) = 0$. We say that $X$ and $Y$ are <em>uncorrelated</em>.</p>
<p>We have shown that independent random variables are uncorrelated. But it is not true that uncorrelated random variables have to be independent. You will show this in an exercise.</p></div></div></div>