<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/14_04_The_Sample_Mean.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Sample-Mean">The Sample Mean<a class="anchor-link" href="#The-Sample-Mean">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What's central about the Central Limit Theorem? One answer is that it allows us to make inferences based on random samples even when we don't know much about the distribution of the population.</p>
<p>In Data 8 you saw that if we want estimate the mean of a population, we can construct confidence intervals for the parameter based on the mean of a large random sample. In that course you used the bootstrap to generate an empirical distribution of the sample mean, and then used the empirical distribution to create the confidence interval. You will recall that those empirical distributions were invariably bell shaped.</p>
<p>In this section we will study the probability distribution of the sample mean and show that you can use it to construct confidence intervals for the population mean without any resampling.</p>
<p>Let's start with the sample sum, which we now understand well. Recall our assumptions and notation:</p>
<p>Let $X_1, X_2, \ldots, X_n$ be an i.i.d. sample, and let each $X_i$ have mean $\mu$ and $SD$ $\sigma$. Let $S_n$ be the sample sum, that is, $S_n = \sum_{i=1}^n X_i$. We know that</p>
$$
E(S_n) = n\mu ~~~~~~~~~~  SD(S_n) = \sqrt{n}\sigma
$$<p>These results imply that as the sample size increases, the distribution of the sample sum moves to the right and becomes more spread out.</p>
<p>You can see this in the graph below. The graph shows the distributions of the sum of 5 rolls and the sum of 20 rolls of a die. The distributions are exact, calculated using the function <code>dist_sum</code> defined using pgf methods earlier in this chapter.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">die</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
<span class="n">dist_sum_5</span> <span class="o">=</span> <span class="n">dist_sum</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">die</span><span class="p">)</span>
<span class="n">dist_sum_20</span> <span class="o">=</span> <span class="n">dist_sum</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">die</span><span class="p">)</span>
<span class="n">Plots</span><span class="p">(</span><span class="s1">'Sum of 5 dice'</span><span class="p">,</span> <span class="n">dist_sum_5</span><span class="p">,</span> <span class="s1">'Sum of 20 dice'</span><span class="p">,</span> <span class="n">dist_sum_20</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/14_04_The_Sample_Mean_4_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see the normal distribution appearing already for the sum of 5 and 20 dice.</p>
<p>You can see also that the gold distribution isn't four times as spread out as the blue, though the sample size in the gold distribution is four times that of the blue. The gold distribution is is half as tall and twice as spread out as the blue. That is because the SD of the sum is proportional to $\sqrt{n}$. It grows slower than $n$. Because the sample size is larger by a factor of 4, the SD of the gold distribution is $\sqrt{4} = 2$ times the SD of the blue.</p>
<p>The <em>average</em> of the sample behaves differently.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Mean-of-an-IID-Sample">The Mean of an IID Sample<a class="anchor-link" href="#The-Mean-of-an-IID-Sample">¶</a></h3><p>Let $\bar{X}_n$ be the sample mean, that is,</p>
$$
\bar{X}_n = \frac{S_n}{n}
$$<p>Then $\bar{X}_n$ is just a linear transformation of $S_n$. So</p>
$$
E(\bar{X}_n) = \frac{E(S_n)}{n} = \frac{n\mu}{n} = \mu ~~~~ \text{for all }n
$$<p>The expectation of the sample mean is always the underlying population mean $\mu$, no matter what the sample size. Therefore, no matter what the sample size, the sample mean is an unbiased estimator of the population mean.</p>
<p>The SD of the sample mean is</p>
$$
SD(\bar{X}_n) = \frac{SD(S_n)}{n} = \frac{\sqrt{n}\sigma}{n} = \frac{\sigma}{\sqrt{n}}
$$<p>The variability of the sample mean decreases as the sample size increases. So, as the sample size increases, the sample mean becomes a more accurate estimator of the population mean.</p>
<p>The graph below shows the distributions of the means of 5 rolls of a die and of 20 rolls. Both are centered at 3.5 but the distribution of the mean of the larger sample is narrower. You saw this frequently in Data 8: as the sample size increases, the distribution of the sample mean gets more concentrated around the population mean.</p></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/14_04_The_Sample_Mean_7_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Accuracy doesn't come cheap. The SD of the sample mean decreases according to the square root of the sample size. Therefore if you want to decrease the SD of the sample mean by a factor of 3, you have to increase the sample size by a factor of $3^2 = 9$.</p>
<p>The general result is usually stated in the reverse.</p>
<h4 id="Square-Root-Law">Square Root Law<a class="anchor-link" href="#Square-Root-Law">¶</a></h4><p>If you multiply the sample size by a factor, then the SD of the sample mean decreases by the square root of the factor.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Weak-Law-of-Large-Numbers">Weak Law of Large Numbers<a class="anchor-link" href="#Weak-Law-of-Large-Numbers">¶</a></h3><p>The sample mean is an unbiased estimator of the population mean, and has a small SD when the sample size is large. So the mean of a large sample is close to the population mean with high probability.</p>
<p>The formal result is called the <em>Weak Law of Large Numbers</em>.</p>
<p>Let $X_1, X_2, \ldots, X_n$ be i.i.d., each with mean $\mu$ and SD $\sigma$, and let $\bar{X}_n$ be the sample mean. Fix any number $\epsilon &gt; 0$; it is best to imagine $\epsilon$ to be very small. Then</p>
$$
P(|\bar{X}_n - \mu| &lt; \epsilon) \to 1 ~~~ \text{as } n \to \infty
$$<p>That is, for large $n$ it is almost certain that the sample average is in the range $\mu \pm \epsilon$.</p>
<p>To prove the law, we will show that $P(|\bar{X}_n - \mu| \ge \epsilon) \to 0$. This is straightforward by Chebyshev's Inequality.</p>
$$
P(|\bar{X}_n - \mu| \ge \epsilon)~ \le ~ \frac{\sigma_{\bar{X}_n}^2}{\epsilon^2} 
~ = ~ \frac{\sigma^2}{n\epsilon^2} ~ \to ~ 0 ~~~ \text{as } n \to \infty
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Related-Laws">Related Laws<a class="anchor-link" href="#Related-Laws">¶</a></h3><ul>
<li><strong>Strong Law of Large Numbers.</strong> This says that with probability 1, the sample average converges to a limit, and that limit is the constant $\mu$. See <a href="https://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/">this blog post by Fields Medalist Terence Tao</a>. He states the laws in the case where the underlying SDs may not exist. Note that our proof of the Weak Law is not valid in that case; the result is still true but the proof needs more care.</li>
<li><strong>Law of Small Numbers.</strong> This is the title of a book by <a href="https://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz">Ladislaus Bortkiewicz</a> (1868-1931) in which he described the Poisson approximation to distributions of rare events. That's why Section 6.4 of these notes is called the Law of Small Numbers.</li>
<li><strong>Law of Averages.</strong> This is a common name for the Weak Law in the case where the population is binary and the sample mean is just the proportion of successes in the sample. In common usage, people sometimes forget that the law is a limit statement. If you are tossing a fair coin and have seen 10 heads in a row, the chance that the next toss is a head is still 1/2. The law of averages does not say that you are "due for a tail". It doesn't apply to finite sets of tosses.</li>
</ul></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Shape-of-the-Distribution">The Shape of the Distribution<a class="anchor-link" href="#The-Shape-of-the-Distribution">¶</a></h3><p>The Central Limit Theorem tells us that for large samples, the distribution of the sample mean is roughly normal. The sample mean is a linear transformation of the the sample sum. So if the distribution of the sample sum is roughly normal, the distribution of the sample mean is roughly normal as well, though with different parameters. Specifically, for large $n$,</p>
$$
P(\bar{X}_n \le x) ~ \approx ~ \Phi \big{(} \frac{x - \mu}{\sigma/\sqrt{n}} \big{)} ~~~~ \text{for all } x
$$</div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/14_04_The_Sample_Mean_12_0.png"/></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>