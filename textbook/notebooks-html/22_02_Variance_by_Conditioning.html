<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/22_02_Variance_by_Conditioning.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Variance-by-Conditioning">Variance by Conditioning<a class="anchor-link" href="#Variance-by-Conditioning">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Iteration allows us to find expectation by conditioning. We now have the tools to find variance by conditioning as well.</p>
<p>Recall the notation of the previous section:</p>
<ul>
<li>$X$ and $Y$ are jointly distributed random variables</li>
<li>$b(X) = E(Y \mid X)$</li>
<li>$D_w = Y - b(X)$</li>
</ul>
<p>Define $D_Y = Y - E(Y)$. Then</p>
$$
D_Y ~ = ~  D_w + (b(X) - E(Y)) ~ = ~ D_w + D_b
$$<p>where $D_b = b(X) - E(Y)$ is the deviation of the random variable $b(X)$ from its expectation $E(Y)$.</p>
<p>In the graph below, the black line is at the level $E(Y)$, and the dark blue point is a generic point $(X, Y)$ in the scatter plot. Its distance from the black line is $D_Y$ and is equal to the sum of two lengths:</p>
<ul>
<li>$D_w$, the length of the purple segment</li>
<li>$D_b$, the length of the green segment</li>
</ul></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/22_02_Variance_by_Conditioning_5_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decomposition-of-Variance">Decomposition of Variance<a class="anchor-link" href="#Decomposition-of-Variance">¶</a></h3><p>The expectation $E(Y)$ is a constant. That means $D_b = b(X) - E(Y)$ is a function of $X$ and hence is uncorrelated with $D_w$. Because $D_Y = D_w + D_b$, we have a <em>decomposition of variance</em>:</p>
$$
Var(D_Y) ~ = ~ Var(D_w) + Var(D_b)
$$<p>Let's take a closer look at these three variances. Shifting a random variable by a constant doesn't affect variance. So:</p>
<ul>
<li>$Var(D_Y) = Var(Y - E(Y)) = Var(Y)$</li>
<li>$Var(D_b) = Var(b(X) - E(Y)) = Var(b(X))$, the <em>variance of the conditional expectation</em>.</li>
</ul>
<p>Finally, because $E(D_w) = 0$,</p>
\begin{align*}
Var(D_w) ~ &amp;= ~ E(D_w^2) \\
&amp;= ~ E\big{(} (Y - b(X))^2 \big{)} \\
&amp;= ~ E\big{(} E\big{(} (Y - b(X))^2 \mid X \big{)} \big{)}
\end{align*}<p>Because $b(X) = E(Y \mid X)$, the random variable $E\big{(} (Y - b(X))^2 \mid X \big{)}$ is a function of $X$ called the <em>conditional variance of $Y$ given $X$</em> and denoted $Var(Y \mid X)$. Its value at $x$ is $Var(Y \mid X=x)$, that is, the variance of the values of $Y$ in the vertical strip at $x$.</p>
<p>So $Var(D_w) = E(Var(Y \mid X))$ is the <em>expectation of the conditional variance</em>.</p>
<p>Because of these observations, the variance decomposition above is more commonly written as a decomposition of the variance of $Y$:</p>
$$
Var(Y) ~ = ~ E(Var(Y \mid X)) + Var(E(Y \mid X))
$$<p>That is, <strong>the variance is equal to the expectation of the conditional variance plus the variance of the conditional expectation</strong>.</p>
<p>It makes sense that the two quantities on the right hand side are involved in the calculation of $Var(Y)$. The variability of $Y$ has two components:</p>
<ul>
<li>the rough size of the variability within the individual vertical strips, that is, the expectation of the conditional variance</li>
<li>the variability between strips, measured by the variance of the centers of the strips.</li>
</ul>
<p>The variance decomposition show that you can just add the two terms to get $Var(Y)$.</p>
<p>This decomposition is the basis of <em>analysis of variance</em> (ANOVA), widely used in statistics. In this course we are going to use it to find variances by conditioning.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>