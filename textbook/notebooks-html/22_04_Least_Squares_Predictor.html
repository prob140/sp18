<div id="ipython-notebook">
            <a class="interact-button" href="http://prob140.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/prob140/materials&branch=gh-pages&subPath=textbook/22_04_Least_Squares_Predictor.ipynb">Interact</a>
            
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Least-Squares-Predictor">Least Squares Predictor<a class="anchor-link" href="#Least-Squares-Predictor">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As the function that picks off the "centers of vertical strips," the conditional expectation $b(X) = E(Y \mid X)$ is a natural estimate or predictor of $Y$ given the value of $X$. We will now see how good $b(X)$ is if we use mean squared error as our criterion.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Minimizing-the-MSE">Minimizing the MSE<a class="anchor-link" href="#Minimizing-the-MSE">¶</a></h3><p>Let $h(X)$ be any function of $X$, and consider using $h(X)$ to predict $Y$. Define the <em>mean squared error of the predictor $h(X)$</em> to be</p>
$$
MSE(h) ~ = ~ E\Big{(}\big{(}Y - h(X)\big{)}^2\Big{)}
$$<p>We will show that $b(X)$ is the best predictor of $Y$ based on $X$, in the sense that it minimizes this mean squared error over all functions $h(X)$.</p>
<p>Recall our notation $D_w = Y - b(X)$. Earlier we showed that:</p>
<ul>
<li>$E(D_w) = 0$ </li>
<li>$D_w$ is uncorrelated with any function of $X$. Therefore if $g(X)$ is any function of $X$, then $E\big{(}D_wg(X)\big{)} = 0$.</li>
</ul></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align*}
MSE(h) ~ &amp;= ~ E\Big{(}\big{(}Y - h(X)\big{)}^2\Big{)} \\
&amp;= ~ E\Big{(}\big{(} (Y - b(X)) + (b(X) - h(X) \big{)}^2 \Big{)} \\
&amp;= ~ E\Big{(}\big{(}Y - b(X)\big{)}^2\Big{)} + E\Big{(}\big{(}b(X) - h(X)\big{)}^2\Big{)} + 2E\Big{(}D_w\big{(}b(X) - h(X)\big{)}\Big{)} \\
&amp;= ~ E\Big{(}\big{(}Y - b(X)\big{)}^2\Big{)} + E\Big{(}\big{(}b(X) - h(X)\big{)}^2\Big{)} \\
&amp;\ge ~ E\Big{(}\big{(}Y - b(X)\big{)}^2\Big{)} \\
&amp;= ~ MSE(b)
\end{align*}</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Best-Predictor">Best Predictor<a class="anchor-link" href="#Best-Predictor">¶</a></h3><p>The result above shows that the least squares predictor of $Y$ based on $X$ is the conditional expectation $b(X) = E(Y \mid X)$.</p>
<p>In terms of the scatter diagram of observed values of $X$ and $Y$, the result is saying that the best predictor of $Y$ given $X$, by the criterion of smallest mean squared error, is the average of the vertical strip at the given value of $X$.</p>
<p>Given $X$, the root mean squared error of this estimate is the <em>SD of the strip</em>, that is, the conditional SD of $Y$ given $X$:</p>
$$
SD(Y \mid X) ~ = ~ \sqrt{Var(Y \mid X)}
$$<p>This is a random variable; its value measures the variability within the strip at the given value of $X$.</p>
<p>Overall across the entire scatter diagram, the root mean squared error of the estimate $E(Y \mid X)$ is</p>
$$
RMSE(b) ~ = ~ \sqrt{E\Big{(}\big{(}Y - b(X)\big{)}^2\Big{)}} ~ = ~ \sqrt{E\big{(} Var(Y \mid X) \big{)}}
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that the result makes no assumption about the joint distribution of $X$ and $Y$. The scatter diagram of the generated $(X, Y)$ points can have any arbitrary shape.</p>
<p>It seems as though the question of prediction has been settled once and for all: if you want the least squares predictor, use conditional expectation. However, the functional form of the conditional expectation of $Y$ given $X$ depends on the joint distribution of $X$ and $Y$ (which also determines the shape of the scatter diagram), and is not always straightforward to find.</p>
<p>So data scientists also find least squares estimates among smaller classes of estimates, the most common class being the set of linear functions of the given variable. This is called linear regression and is the topic of a later chapter.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>